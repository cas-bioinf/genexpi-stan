---
title: "ODE Model of Gene Regulation"
output: 
  html_document:
      fig_caption: TRUE
      code_folding: hide
author: | 
  | Martin Modr√°k
  | Laboratory of Bioinformatics
  | Institute of Microbiology of the Czech Academy of Sciences
  | martin.modrak@biomed.cas.cz
bibliography: stancon.bib
---

```{r setup, message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(fig.height=3, fig.path='Figs/',
               echo=TRUE, warning=FALSE, message=FALSE)

#In addition to the libraries listed below,, the Genexpi-stan pacakge requires:
# rstan, deSolve, splines, truncnorm, parallel, loo
devtools::install(args = "--no-multiarch")  #I need to build the package (and cannot use load_all) to make the functions in the package available when running in parallel).
library(genexpiStan)
library(cowplot)
library(tidyverse)
library(here)
library(MVN)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```


# Abstract

In determining regulatory interactions within cells, models based on ordinary differential equations (ODE) are a popular choice. Here, we present a reimplementation of one such model in Stan. The model features a spline fit where the resulting spline is a parameter of the ODE. For practical reasons, the model avoids the Stan's ODE solver in favor of a custom solution. We also discuss why the model as traditionally formulated is not well identified and introduce reparametrizations that improve identifiability and make it easier to specify reasonable default priors.

This notebook shows first usable version of the model, but there are still several outstanding issues which we highlight.

# Biological Background
In a very simplified form the [central dogma](https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology) of molecular biology may be paraphrased as follows: The DNA contains all of the instructions needed to run a cell and can be divided into coding and non-coding regions. The coding regions of DNA can be transcribed to form messenger RNA (mRNA) molecule containing a mirror-copy of the coding region. The messenger RNA is then translated to create protein. Proteins then perform most of the actual functions of the cells, including control of transcription and translation. This process is regulated at all stages and involves many feedback loops: most importantly, the rate of transcription of a gene is controlled by the abundance of regulatory proteins binding to specific sequences of the DNA near the coding region. The rate of translation and degradation of mRNA can also be regulated separately by other proteins and the rate of degradation of proteins themselves is also affected by other proteins. There are many exceptions where the above simplification does not hold, but the vast majority of cellular life can be described in these terms. 

Understanding what are the regulations taking place is thus an important step in understanding how cells work. However, our ability to observe what happens in the cell is very limited: The only commonly available method that can capture information about all genes in a single experiment is measuring the concentration of mRNA which is in turn strongly related to expression (how many mRNAs are transcribed from the gene in a unit of time). Gathering expression data remains relatively expensive and except for the most studied organisms, at most several dozen whole genome expression experiments have been published.

It has been shown that mRNA and protein concentration are mostly correlated, however this relationship is imperfect [@Maier09,@Gygi99]. Nevertheless, expression data is the best proxy for protein concentration available at the whole genome level and expression data are thus widely used to infer interactions that control transcription of genes.

# Scope

The model presented in this notebook aims at modelling and identifying transcriptional regulations from time series data of gene expression. While the model aims to be more general, our particular the interest is on determining for which genes does a known [*sigma factor*](https://en.wikipedia.org/wiki/Sigma_factor) initiate transcription.

The present work was motivated by reading [@TitsiasHonkela12] who developed a fully Bayesian model of transcriptional regulation, extending a classical regulation model [@Vohradsky2001] with Gaussian Processes. Titsias et al. used a custom Monte Carlo sampler and to our knowledge, there is no other fully Bayesian treatment of the model available. Initially, we tried to directly rephrase the Titsias et al. model in Stan, but we ran into large computational difficulties, including several non-identifiabilities, that we were not able to resolve without modifying the model. In this notebook, we present a model that resulted from resolving those issues and a slight simplification of the original model, as our use case didn't need to model protein dynamics separately. We further show how we determine model fit and use a simple auxiliary expression model to filter out genes whose expression does not identify the parameters of the full model.

# Data

The data we will work with in this notebook is a time series of 14 microarray measurements of expression of 4008 genes in the bacterium *Bacillus subtilis* during germination (transition from dormant spores to normal metabolism) [@Keijser2007], depositioned in the Gene Expression Omnibus under GSE6865. (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE6865).

```{r}
#Try to download the data, if it was not already present on the computer

data_dir <- here("local_data")
if(! dir.exists(data_dir)) {
  dir.create(data_dir)
}
data_file <- here("local_data","GSE6865_series_matrix.txt.gz")
if(!file.exists(data_file)) {
  download.file("ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE6nnn/GSE6865/matrix/GSE6865_series_matrix.txt.gz", data_file)
}

# Read and preprocess the data
gse6865_raw_df = read.delim(gzfile(data_file), comment.char = "!") #Intermediate data frame representation

# Raw profile data. We scale by 1000 to get to more wieldy values
gse6865_raw = as.matrix(gse6865_raw_df[,2:15]) / 1000
rownames(gse6865_raw) = gse6865_raw_df$ID_REF

#Times (in minutes) for the individual samples
gse6865_raw_time = c(0,5,10,15,20,25,30,40,50,60,70,80,90,100)
colnames(gse6865_raw) <- sapply(gse6865_raw_time,  FUN = function(x) { paste0(x,"min")})

#There are a few NA values in 1st and 2nd measurement, which we will ge trid of.

# For the first measurement, we can expect the value to be 0 (the series are from germination)
gse6865_raw[is.na(gse6865_raw[,1]),1] = 0

# For the second measurement we will go with a linear interpolation
na2 = is.na(gse6865_raw[,2])
gse6865_raw[na2,2] = 0.5 * gse6865_raw[na2,1] + 0.5 * gse6865_raw[na2,3]

#cleanup the intermediate results
rm(gse6865_raw_df)
rm(na2)
rm(data_dir)
rm(data_file)
```

Below is a sample of the expression profiles of 20 random genes, let us notice that most genes have almost zero expression.

```{r}
set.seed(2345277)
genes_to_show <- sample(1:nrow(gse6865_raw), 20)
ggmatplot(gse6865_raw_time, t(gse6865_raw[genes_to_show,]), main_geom = geom_line(alpha = 0.6), x_title = "Time [min]", y_title = "expression")
```


# Basics of The Model

In all of the following, $x_i$ is the true (unknown) expression of a target gene,$y_j$ the true protein concentration of a regulator and $\rho_i$ the regulatory input. Both $x_i$, $y_j$ and $\rho_i$ are functions of time while all the other parameters are constants. The regulatory input is a linear combination of protein concentrations of the regulators:

$$
\rho_i = \sum_j{w_{i,j}y_j} + b_i
$$
Then the expression of $x_i$ is driven by the following ordinary differential equation (ODE):
$$
\begin{align}
\frac{\mathrm{d}x_i}{\mathrm{d}t} &= s_i F(\rho_i) - d_i x_i \\
F(\rho_i) &= \frac{1}{1 + e^{-\rho_i}}
\end{align}
$$


where $F$ is the regulatory response function, in our case the logistic sigmoid.

Our goal is, to use Stan to infere the parameters of this model ($s_i,w_{i,j},b_i, d_i$), given the observed expression of the target genes ($x_i$) and regulators ($y_i$) over time.

There are two forms of expression data available with widely different noise models: [microarray](https://en.wikipedia.org/wiki/DNA_microarray) and [RNA-seq](https://en.wikipedia.org/wiki/RNA-Seq). The former produces positive continuous measurements with approximately normal error while the latter produces count data with negative-binomial distribution. The datasets we are interested in come from microarray experiments so we will focus on those, but using RNA-seq means just changing the observation model. For microarrays, the observation model can be treated as a truncated normal:

$$
\tilde{x_i}(t) \sim N(x_i(t), \sigma_{abs} + x_i(t)\sigma_{rel})  \mathop{|} \tilde{x_i}(t) > 0
$$

The two error terms represent the fixed absolute error ($\sigma_{abs}$) which is the result of technical noise in the microarray platform and the relative error ($\sigma_{rel}$) wich represents the uncaptured biological variation which tend to be proportional to the expresssion of the genes. 

For the regulator(s) there are two possible regimes: a) the expression of the regulators is observed and the protein concentration is assumed to be indentical to the true expression or b) the $y$ is estimated only through its influence on $x$. In case a), the exact same error model applies to regulators.


In the following, we treat both $\sigma_{abs}$ and $\sigma_{rel}$ as given. The model can treat the error terms as parameters, but with the dataset we work with, the resulting $\sigma_{rel}$ values tended to be unrealistically high and most fits unexpectedly poor as a consequence. We are yet to investigate why this is the case.

[@Klebanov2007]
suggests that technical variation is constant and expects $\sigma_{abs} = 0.1$. Based on our previous experience with similar models, we expect the biological variation to be around 20% of the expression, suggesting $\sigma_{rel} = 0.1$.



# Modelling Regulator Expression

To solve the regulation ODE numerically, we need to have $y_j$ available at much finer intervals than the available measurements (1-2 minute intervals have proven sufficient). Initially, we considered Gaussian Processes [as @TitsiasHonkela12]), but those introduced computational issues and we switched to B-splines which have less theoretical appeal, but introduce fewer parameters and were easier to fit.

We however need to ensure that $y_j$ are positive. After some early setbacks with the ```log1pexp`` transform ($x^+ = ln(1+e^x)$), we settled to simply subtract the minimum of the spline from all points[^1], i.e. given the spline basis $B$ we get:

$$
\begin{aligned}
\bar{y_j} &= B\alpha_j \\
y_j &= c_{scale}(\bar{y_j} - \min{\bar{y_j}}) + \beta_j \\
\alpha_j &\sim N(0,1) \\
\beta_j &\sim HalfNormal(0, \sigma_\beta)
\end{aligned}
$$

Where $\alpha_j$ is a column vector of spline coefficients and $\beta_j$ serves as intercept, all are treated as parameters. The scaling constant $c_{scale}$ and $\sigma_\beta$ reflect the range of the data and are given by the user. When the regulator proteins are estimated from the effect on targets only (no expression measurements for regulator), the spline intercept $\beta_j$ is ignored, as it cannot be separated from $b_i$.


[^1]: We now believe that the initial problems were not caused by ```log1pexp``` but by other parts of the model, however, the $\min$ transform is working well and there is currently no incentive to replace it. Note that $\min{\bar{y_i}}$ is mostly a smooth function of $\alpha_i$.

# Solving the ODE

It is a bit tricky to use Stan's built-in ODE solver with a spline as one of the parameters driving ODE evolution as it needs the value of $y_i$ at arbitrary time points. The most straightforward way - linearly interpolating between precomputed discrete values of $y_i$ requires some hacks and breaks the solver. In principle, $\alpha_i$ could be given to the ODE solver and the spline basis computed for each timepoint the solver needs, but this seemed cumbersome. Instead we decided to follow the approach of [@TitsiasHonkela12]. First we can observe that the regulation ODE is linear in $x_i$ and can be solved for $t \geq 0$:

$$
x_i(t) = \eta_i e^{-d_i t} + s_i \int_0^t F(\rho_i) e^{-d_i(t-u)} \mathrm{d}u
$$
Where $\eta_i$ is the concentration of the target at $t=0$. We then solve the resulting integral numerically via [trapezoid rule](https://en.wikipedia.org/wiki/Trapezoidal_rule)[^2]. Assuming unit timestep, $x_i$ can be computed in a single for loop:

$$
\begin{aligned}
\chi_i(0) &= -\frac{1}{2} F(\rho_i(0)) \\
\chi_i(t + 1) &= (\chi_i(t) + F(\rho_i(t)))d_i \\
x_i(t) &= \eta_i e^{-d_i t} + s_i(\chi_i(t) + \frac{1}{2}F(\rho_i(t)))
\end{aligned}
$$

[^2]: Trapezoid rule agrees well with the results of solving the ODE with RK45, while the simpler midpoint rule has significant bias even when the time steps are small.

# Identifiability

The model as given above is biologically relevant, but has multiple identifiability issues, all of which can be demonstrated with just a single regulator and a single target, so we will drop parameter indicies for the rest of this section. The first non-identifiability arises with $w$ and $b$ if all values of $\rho$ fall on the tail of the sigmoid, which is approximately linear. In this case, the walue of $F(\rho)$ becomes insensitive to linear transformations of $w$ and $b$:

```{r}
time <- seq(0,2,length.out = 100)
regulator <- sin(time * 4) + 1

params1 <- c(degradation = 0.5, bias = -3, sensitivity = 1, weight = 5, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target1 <-  ode( y = c(x = 0), times = time, func = target_ODE, parms = params1, method = "ode45")[,"x"];

params2 <- c(degradation = 0.5, bias = -6, sensitivity = 1, weight = 10, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target2 <-  ode( y = c(x = 0), times = time, func = target_ODE, parms = params2, method = "ode45")[,"x"];

params3 <- c(degradation = 0.5, bias = -30, sensitivity = 1, weight = 50, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target3 <-  ode( y = c(x = 0), times = time, func = target_ODE, parms = params3, method = "ode45")[,"x"];

params_to_legend <- function(params) {
  paste0("w = ", params["weight"], ", b = ", params["bias"])
}

data.frame(time = time, regulator = regulator, target1 = target1, target2 = target2, target3 = target3) %>%
  gather("profile","expression", -time) %>%
  ggplot(aes(x = time, y = expression, color = profile)) + 
  geom_line() + 
  scale_color_hue(labels = c("regulator", params_to_legend(params1), params_to_legend(params2), params_to_legend(params3))) + ggtitle("Non-identifiability due to w and b","All targets have s = 1, d = 0.5")
```
Transformations of $s$ and $d$ together may also have very minor effect on the resulting expression. While the resulting expression profiles are more different than in the previous case, those differences are still very small compared to the noise observed in real data:
```{r, fig.cap="The red dots represent measurements that have approximately equal likelihood for each of the shown true target profiles, assuming $\\sigma \\simeq 0.5$"}
time <- seq(0,2,length.out = 100)
regulator <- sin(time * 4) + 1

params1 <- c(degradation = 5, bias = -1, sensitivity = 10, weight = 1, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target1 <-  ode( y = c(x = 1), times = time, func = target_ODE, parms = params1, method = "ode45")[,"x"];

params2 <- c(degradation = 10, bias = -1, sensitivity = 19, weight = 1, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target2 <-  ode( y = c(x = 1), times = time, func = target_ODE, parms = params2, method = "ode45")[,"x"];

params3 <- c(degradation = 100, bias = -1, sensitivity = 180, weight = 1, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target3 <-  ode( y = c(x = 1), times = time, func = target_ODE, parms = params3, method = "ode45")[,"x"];

params_to_legend <- function(params) {
  paste0("s = ", params["sensitivity"], ", d = ", params["degradation"])
}

measured_data = data.frame(profile = "measured", time = seq(0,2,length.out = 9), 
                           expression = c(1,1.7,1.4,0.85, 0.83,0.35,0.7,1.4,1.5))


data.frame(time = time, regulator = regulator, target1 = target1, target2 = target2, target3 = target3) %>%
  gather("profile","expression", -time) %>%
  ggplot(aes(x = time, y = expression, color = profile)) + geom_line() + 
  geom_point(data = measured_data, color = "#ba1b1d", size = 3) +
  scale_color_hue(labels = c("regulator",params_to_legend(params1), params_to_legend(params2), params_to_legend(params3))) + ggtitle("Non identifiability due to s and d","All targets have w = 1, b = -1")
```
There is also a non-identifiability when $w = 0$ as $s$ and $b$ become redundant. Even worse, some of the solutions with $w = 0$ might not be distinguishable form solutions with $|w| \gg 0$. This might also induce non-identifiability in the initial condition $\eta$ as it is much less constrained by data. Once again the expression profiles are not identical, but are close enough that given the amount of noise, non-negligible posterior mass can be found for all of them:

```{r, fig.cap="The red dots represent measurements that have approximately equal likelihood for each of the shown true target profiles, assuming $\\sigma \\simeq 0.5$"}
time <- seq(0,2,length.out = 100)
regulator <- sin(time * 4) + 1

params1 <- c(degradation = 3, bias = -1, sensitivity = 3, weight = 5, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target1 <-  ode( y = c(x = 1.5), times = time, func = target_ODE, parms = params1, method = "ode45")[,"x"];

params2 <- c(degradation = 3.4, bias = 0, sensitivity = 6, weight = 0, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target2 <-  ode( y = c(x = 1.6), times = time, func = target_ODE, parms = params2, method = "ode45")[,"x"];

params3 <- c(degradation = 10, bias = 0, sensitivity = 16, weight = 0, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target3 <-  ode( y = c(x = 2.5), times = time, func = target_ODE, parms = params3, method = "ode45")[,"x"];


params_to_legend <- function(params) {
  paste0("s = ", params["sensitivity"],", w = ", params["weight"], ", b = ", params["bias"], ", d = ", params["degradation"])
}

measured_data = data.frame(profile = "measured", time = seq(0,2,length.out = 9), 
                           expression = c(1.8,0.9,1,0.66, 1.11,0.6,0.7,0.96,1.03))


data.frame(time = time, regulator = regulator, target1 = target1, target2 = target2, target3 = target3) %>%
  gather("profile","expression", -time) %>%
  ggplot(aes(x = time, y = expression, color = profile)) + geom_line() + 
  geom_point(data = measured_data, color = "#ba1b1d", size = 3) +
  scale_color_hue(labels = c("regulator",params_to_legend(params1), params_to_legend(params2), params_to_legend(params3))) + ggtitle("Non identifiability between w=0 and w > 0")
```
Last but not least, it is also possible to get similar solutions with different sign of $w$:

```{r, fig.cap="The red dots represent measurements that have approximately equal likelihood for each of the shown true target profiles, assuming $\\sigma \\simeq 0.5$"}
time <- seq(0,2,length.out = 100)
regulator <- (sin(time * 1.5) ^ 2) * 2

params1 <- c(degradation = 2, bias = -1, sensitivity = 3, weight = 5, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target1 <-  ode( y = c(x = 0), times = time, func = target_ODE, parms = params1, method = "ode45")[,"x"];

params2 <- c(degradation = 1.4, bias = 10, sensitivity = 2.5, weight = -5, basal_transcription = 0, protein = approxfun(time, regulator, rule=2));
target2 <-  ode( y = c(x = 0), times = time, func = target_ODE, parms = params2, method = "ode45")[,"x"];


params_to_legend <- function(params) {
  paste0("s = ", params["sensitivity"],", w = ", params["weight"], ", b = ", params["bias"], ", d = ", params["degradation"])
}

measured_data = data.frame(profile = "measured", time = seq(0,2,length.out = 9), 
                           expression = c(0.2,0.35,0.61,1.03, 1.11,1.42,1.07,1.76,1.23))


data.frame(time = time, regulator = regulator, target1 = target1, target2 = target2) %>%
  gather("profile","expression", -time) %>%
  ggplot(aes(x = time, y = expression, color = profile)) + geom_line() + 
  geom_point(data = measured_data, color = "#ba1b1d", size = 3) +
  scale_color_hue(labels = c("regulator",params_to_legend(params1), params_to_legend(params2))) + ggtitle("Non identifiability between w < 0 and w > 0")
```

There are probably many more non-identified scenarios, especially when multiple targets targets are involved, but the above are of the type we have actually encountered with real data.

## Reparametrization

First we realized that when there is a good fit with $w < 0$ and a similarly good one with $w > 0$, there is nothing we can do to make the model identifiable, so we decided to make the user specify the signs of regulatory interactions $I_{i,j} = sgn(w_{i,j})$  as data. This is not a big hindrance as this type of regulation model is mostly employed to test direct regulations by sigma factors which are expected to always have $w > 0$. If the sign of $w$ is important and there is only one regulator, it is possible to fit each target separately with different values of $I_{1,1}$. The only truly problematic case is when fitting a model with multiple regulators and unknown $I$, but since two regulators already often overfit on most practical datasets this is of little practical concern.

Some of the non-identifiabilities are a result of the fact, that the roles the parameters play in the model are partially shared by muliple parameters. To get rid of those complex dependencies, we introduced several reparametrizations. The only parameter we kept directly is the degradation parameter $d_i$, which also has the nice property that it does not depend on the scale of the expression data (only on the length of the timestep).

Instead of $s_i,b_i$ and $w_{i,j}$, we introduce $\mu^{\rho}_i$, the mean regulatory input, $\sigma^{\rho}_i$, the sd of the regulatory input, $\gamma_i$ a simplex of relative regulator weights and $a_i$ the asymptotic normalized state. All of those parameters are also decoupled from the actual values of the expression data. The new parameters are connected with the original parameters as follows:

$$
\begin{align}
w_{i,j} &= I_{i,j} \gamma_{i,j} \frac{\sigma^{\rho}_i}{\mathrm{sd}(y_j)} \\
b_i &= \mu^{\rho}_i - \sum_j w_{i,j} E(y_j) \\
s_i &= \frac{a_i d_i \max\tilde{x_i} }{ E(F(\rho_i)) } \\

\sum_j \gamma_{i,j} &= 1 , & \gamma_{i,j} > 0 \\
\end{align}
$$
Where $E$ and $\mathrm{sd}$ correspond to the sample mean and standard deviation. The formula for $w_{i,j}$ assumes independence among the regulators which does not hold, but handling the covariance structure correctly would be difficult and could lead to multiple solutions. The interpretion of $a_i$ is that if the expression stays at its mean level, e.g. $F(\rho_i) \simeq E(F(\rho_i))$ and $t \rightarrow \infty$ then $x_i(t) \rightarrow a_i$. Also note that using $\max\tilde{x_i}$ to scale $s_i$ means the model is not completely generative, as we cannot generate $\tilde{x_i}$ before we generate $s_i$, but this has shown to not be a problem in practice.

The reparametrization not only helped identify the posterior but it is also much easier to specify priors for those parameters! We ended up with the following priors:
$$
\begin{align}
\mu^{\rho}_i &\sim N(0,\tau_{\rho,\mu}) &\\
\sigma^{\rho}_i &\sim N(0,\tau_{\rho,\sigma})  &| \sigma^{\rho}_i > 0 \\
a_i &\sim  N(1,\tau_{a})  &| a_i > 0 \\
log(d_i) &\sim N(\nu_d,\tau_d) & 
\end{align}
$$
Where all the hyperparameters are given by user, but sensible defaults can be given as the hyperparameters are decoupled from the scale of the data. Since the sigmoid $F$ is mostly saturated outside $[-5,5]$, we use $\tau_{\rho,\mu} = \tau_{\rho,\sigma} = 5$. Further we expect $a_i$ should lie mostly in $[0,2]$ (e.g. if given more time the gene will unlikely raise to more than twice the maximum observed), giving $\tau_a = \frac{1}{2}$ and finally we expect degradation to be non-negligible but less than 1 (e.g. all mRNA degrading in a single unit of time), giving $\nu_d = -2$ and $\tau_d = 1$.

## The constant synthesis model

To get rid of the non-identifiabilities connected with $w = 0$, we first try to fit a simpler *constant synthesis* model to each target separately, this model is given by:

$$
\frac{\mathrm{d}x_i}{\mathrm{d}t} = s'_i - d'_i x_i
$$
As in the general model, the constant synthesis model is not always well identified with this representation and we thus reparametrize with $d'_i$ and $a'_i$ as the asymptotic normalized state.

$$
s'_i = a'_i d'_i \max\tilde{x_i}
$$
This also lets us use the same priors for $a'_i$ and $d'_i$ as for $a_i$ and $d_i$ respectively.

If the target is "reasonably well" fit with the constant synthesis model, it is not considered for the full model, because it could then be fit by any regulator, simply be setting $w = 0$. The quality of the model fit is assessed with WAIC, see discussion below for further details. Note that the constant synthesis model also fits all lowly expressed genes. To our knowledge, filtering with a such a simpler model was first proposed in our recent work [@Modrak2018].

# Workflow

The assumed workflow when using the model to infer novel regulations proceeds as follows:

1. Gather putative and known targets of the regulators of interest
2. Fit the constant synthesis model to all known & putative targets
3. Assess model fit and discard targets that are fit well
4. *Optional:* Use the known targets to constrain the true expression of the regulators
5. Fit the main model to each putative target separately
6. Compare the fit with the main model to the fit of the constant synthesis model

In the following example, we will use 5 known and 4 putative targets of the *sigA* regulator. In all of the following we use cubic spline with 6 degrees of freedom, $c_scale = 5$ as the basis for the main model.

```{r}
#Globally used params for the algorithm
measurement_times = gse6865_raw_time + 1
smooth_time  <- 1:101#seq(0,100, by = 1)
expression_data <- gse6865_raw

spline_df <- 6
spline_basis <- bs(smooth_time, degree = 3, df = spline_df)
default_spline_params <- spline_params(
  spline_basis = spline_basis,
  scale = 5
)

default_params_prior <-  params_prior(
  initial_condition_prior_sigma = 2,
  asymptotic_normalized_state_prior_sigma = 2,
  degradation_prior_mean = -2,
  degradation_prior_sigma = 1,
  mean_regulatory_input_prior_sigma = 5,
  sd_regulatory_input_prior_sigma =5,
  intercept_prior_sigma = 2
)

default_measurement_sigma = measurement_sigma_given(0.1,0.1)
```

```{r}
putative_targets <- c("kinE","purT", "yhdL", "codV")

#The known targets are those predicted and biologically validated in our previous work with the dataset (https://doi.org/10.1016/j.bbagrm.2017.06.003)
known_targets <- c("acpA","fbaA","rpmGA","ykpA","yyaF")


plot_profiles <- function(expression_data, targets) {
  expression_data[targets,] %>% as.data.frame() %>% 
    rownames_to_column("gene") %>% 
    gather("time","expression",-gene) %>%
    mutate(time = as.integer(gsub("min","",time, fixed = TRUE))) %>%
    ggplot(aes(x = time, y = expression, color = gene, linetype = gene)) + geom_line()
}

plot_profiles(expression_data, putative_targets) + ggtitle("Putative targets")
plot_profiles(expression_data, known_targets) + ggtitle("Known targets")
```


## Fit the constant synthesis model

Below are posterior samples from fitting the constant synthesis model to all of the targets.

In addition to samples from posterior true expression we also show replicates of the measured values, to get a better sense of the level of noise involved.


```{r cache=TRUE}
csynth_model <- stan_model(file = here('constant_synthesis.stan'))
```

```{r,cache=TRUE}

targets <- c(putative_targets, known_targets)
data_csynth <- list()
for(t in 1:length(targets)) {
  data_csynth[[t]] <- list(
    num_measurements = length(measurement_times),
    measurement_times = measurement_times,
    expression = expression_data[targets[t],],
    measurement_sigma_absolute = default_measurement_sigma$sigma_absolute_data[1],
    measurement_sigma_relative = default_measurement_sigma$sigma_relative_data[1],
    initial_condition_prior_sigma = default_params_prior$initial_condition_prior_sigma,
    asymptotic_normalized_state_prior_sigma = default_params_prior$asymptotic_normalized_state_prior_sigma,
    degradation_prior_mean = default_params_prior$degradation_prior_mean,
    degradation_prior_sigma = default_params_prior$degradation_prior_sigma
  )  
}

if(!dir.exists(here("out"))) {
  dir.create(here("out"))
}

results_csynth <- sampling_multi(csynth_model, data_csynth, output.dir = here("out","csynth"))
```

```{r, cache=TRUE}
plots <- list()
for(t in 1:length(targets)) {
  fit <- sampling_multi_read_fit(results_csynth, t)
  plot1 <- fitted_csynth_plot(fit, data_csynth[[t]], name = targets[t])
  plot2 <- fitted_csynth_observed_plot(fit, data_csynth[[t]], name = targets[t])
  plot_grid(plot1, plot2, ncol = 2) %>% print()
}

```


## Assesing constant synthesis fit

But how to determine which genes are "fit well" by the constant synthesis model? The best way would probably be to use leave-one-out crossvalidation, but that is computationally prohibitive as hundreds of genes need to be checked in practice. Instead we approximate the value with WAIC using the ```loo``` package [@looPackage] which is feasible and fits well with our intuition of the ordering of the quality of fits. Here are the WAIC scores for the constant synthesis fits:

```{r,cache=TRUE}
waic_csynth <- get_waic_csynth_multi(results_csynth)
csynth_table <- tibble(target = targets, waic_csynth = waic_csynth)
csynth_table
```

Our workflow currently assumes that a human inspects the fits and specifies a WAIC threshold manually. The threshold can be relatively conservative, the only thing that needs to be avoided are non-identifiabilities due to conflict between fits with $w = 0$ and fits with $w \gg 0$. In this example, we will set threshold to 0, elminating only *kinE* from further consideration.


# Estimating regulator expression from known targets

The motivation for using known regulations is that the data constrain the true expression quite weakly. This is not an issue for the putative targets, but becomes problematic for the expression of the regulators. When trying to determine new regulations, only a single target at a time needs to be fit, because the model assumes that all the regulations actually take place and thus a single regulation that is actually not true may shift the parameter values considerably. While it would be in principle possible to marginalize over the power set of targets to get estimates of which regulations take place in a single fit, this is computationally infeasible. But when fitting each target separately, the estimated expression of regulators may not be consistent across targets, possibly leading to false positives.

Similarly to [@TitsiasHonkela12], our model is able to use known regulations to reduce the uncertainty in regulator expression, hopefully elminating any gross inconsistencies between individual fits for putative targets. This is a simple side-effect of the Bayesian treatment, where after fitting the model with all the known targets at once, we can extract the posterior for $y$ and use it as input when fitting putative targets.

But first, let us see the posterior samples from fitting the regulator with 0 targets (equivalent to simply splining the profile). The red dots are the measured values:


```{r load_model}
#Load the model
multiple_targets_model <- stan_model(file = here('multiple_targets_splines.stan'))
```

```{r spline_only, cache = TRUE, results="hide"}
set.seed(4127785)
source <- "sigA"

data <- regulation_model_params(
  measurement_times = measurement_times,
  regulator_expression = expression_data[source,],
  measurement_sigma = default_measurement_sigma,
  spline_params = default_spline_params,
  params_prior = default_params_prior
)


fit_regulator_spline_only <- sampling(multiple_targets_model, data = data, control = list(adapt_delta = 0.95))

fitted_regulator_plot(fit_regulator_spline_only, data, name = paste0(source, " spline only"))
```

While the exact scaling of the regulator expression should not affect model results much, we see that there is also qualitative uncertainty, for example in the number of local minima and maxima after the 60 minute mark.

And this is what the posterior looks like when both regulator expression and 5 known targets are used. Note that unlike using only splines, the profiles are now very similar especially in that local minima and maxima are at almost the same time point: 
```{r all_info, cache = TRUE, results="hide"}
set.seed(751235428)


data <- regulation_model_params(
  measurement_times = measurement_times,
  regulator_expression = expression_data[source,],
  target_expression = t(expression_data[known_targets,,drop = FALSE]),
  regulation_signs = matrix(1, 1, length(known_targets)),
  measurement_sigma = default_measurement_sigma,
  spline_params = default_spline_params,
  params_prior = default_params_prior
)


fit_regulator_all_info <- sampling(multiple_targets_model, data = data, control = list(adapt_delta = 0.95))

fitted_regulator_plot(fit_regulator_all_info, data, name = paste0(source, " all information"))

# for(target in 1:length(training_targets)) {
#   fitted_target_plot(fit_regulator_all_info, data, target = target, name = training_targets[target]) %>% print()
# }

```

## Transferring the learned expression to other fits

To transfer this "learned" expression of the regulator to other fits, we need to make additional assumptions. The most direct way is to treat the fitted spline coefficients $\alpha_j$ as samples from a multivariete normal (MVN) distribution. Some caution has to be excersised since the distribution of coefficients has heavier tails than expected:

```{r,fig.height=5}
pairs(fit_regulator_all_info, pars = "coeffs")
samples_coeffs <- rstan::extract(fit_regulator_all_info,"coeffs")$coeffs[,,1]
mvn_test_results <- mvn(samples_coeffs, mvnTest = "mardia", desc = FALSE, multivariatePlot = "qq")
mvn_test_results$multivariateNormality
```

The MVN approximation is imperfect, and in practice, we have found that it still gives a lot of leeway for the putative target fits to move away from this estimated distribution as it is only half the data used in the model. To combat this effect, we shrink the "learned" distribution by multiplying the covariance matrix by $0.5$. We currently do not understand deeply why the shrinking leads to better behavior, and it is a simple hack which we should improve upon and provide a more principled way.

```{r, fig.height=5}
n_samples <- 100
mvn_unscaled <- coeffs_prior_from_fit(fit_regulator_all_info, covariance_scale = 1)
cov_scale <- 0.5

means_array <- t(array(
  rep(mvn_unscaled$coeffs_prior_mean,n_samples),c(length(mvn_unscaled$coeffs_prior_mean), n_samples)))


samples_learned <- array(rnorm(length(mvn_unscaled$coeffs_prior_mean) * n_samples, 0, 1), c(n_samples, length(mvn_unscaled$coeffs_prior_mean))) %*% chol(mvn_unscaled$coeffs_prior_cov[1,,]) + means_array

samples_learned_scaled <- array(rnorm(length(mvn_unscaled$coeffs_prior_mean) * n_samples, 0, 1), c(n_samples, length(mvn_unscaled$coeffs_prior_mean))) %*% chol(mvn_unscaled$coeffs_prior_cov[1,,] * cov_scale) + means_array


limits <- ylim(c(0,6.5))
plot1 <- fitted_regulator_plot(fit_regulator_all_info, data, name = paste0(source, " all information"), num_samples = n_samples) + limits

measured_geom <- geom_point(data = data.frame(x = data$measurement_times,  y = data$regulator_expression), aes(x=x, y=y), inherit.aes = FALSE, color = "#ba1b1d", size = 3)

plot2 <- ggmatplot(1:data$num_time, t(samples_learned %*% t(data$spline_basis)) * data$scale, main_geom = default_expression_plot_main_geom) + measured_geom + ggtitle("MVN approximation unscaled") + limits

plot3 <-ggmatplot(1:data$num_time, t(samples_learned_scaled %*% t(data$spline_basis)) * data$scale, main_geom = default_expression_plot_main_geom) + measured_geom  + ggtitle(paste0("MVN approximation, scale = ", cov_scale)) + limits

plot_grid(plot1, plot2, NULL, plot3, nrow = 2)
```



## Fitting the model for putative targets

Now, we can fit the actual model for the remaining three putative targets: 
```{r,cache=TRUE}
targets <- c("purT", "yhdL", "codV")

data_regulated <- list()
coeffs_prior <- coeffs_prior_from_fit(fit_regulator_all_info, covariance_scale = 1)
for(t in 1:length(targets)) {
  data_regulated[[t]] <- regulation_model_params(
    measurement_times = measurement_times,
    target_expression = t(expression_data[targets[t],,drop = FALSE]),
    regulation_signs = matrix(1, 1, 1),
    measurement_sigma = default_measurement_sigma,
    spline_params = default_spline_params,
    params_prior = default_params_prior,
    coeffs_prior = coeffs_prior
  )  
}

if(!dir.exists(here("out"))) {
  dir.create(here("out"))
}

results_regulated <- sampling_multi(multiple_targets_model, data_regulated, output.dir = here("out","regulated"))
```

```{r,cache=TRUE}
for(t in 1:length(targets)) {
  fit <- sampling_multi_read_fit(results_regulated, t)
  plot1 <-  fitted_target_plot(fit, data_regulated[[t]], name = targets[t])
  plot2 <- fitted_target_observed_plot(fit, data_regulated[[t]], name = targets[t])
  plot_grid(plot1, plot2, ncol = 2) %>% print()
}

```

## Comparison to constant synthesis model

We can now use WAIC to compare the fits of the putative regulators to the csynth fits. Once again, in practice, hundreds of putative targets might need to be examined. It is assumed that a human will inspect some of the fits and specify a minimal WAIC improvement over the constant synthesis model and possibly also an absolute WAIC threshold to consider fits adequate. Both visual inspection and the computed WAIC show that *purT* cannot be fit by *sigA* while both *yhdL* and *codV* are fit well.

```{r}
waic_regulated <- get_waic_multiple_targets_multi(results_regulated)
regulated_table <- tibble(target = targets, waic_regulated = waic_regulated) %>%
  left_join(csynth_table, by = "target")
regulated_table
```


# Exploratory analysis with multiple regulators

Since *purT* is likely not regulated by *sigA*, we might try a combination of multiple regulators. Unless the dataset is much larger than the one we are using here, this is highly speculative as many combinations of two regulators are able to fit a majority of all genes quite well.

```{r, cache=TRUE}
regulators = c("sigA", "sigB")
targets = "purT"

data_two_reg <- regulation_model_params(
    measurement_times = measurement_times,
    regulator_expression = t(expression_data[regulators,,drop = FALSE]),
    target_expression = t(expression_data[targets,,drop = FALSE]),
    regulation_signs = matrix(1, 2, 1),
    measurement_sigma = default_measurement_sigma,
    spline_params = default_spline_params,
    params_prior = default_params_prior
  )  

fit_two_reg <- fit_multiple_targets(data_two_reg, multiple_targets_model)
```

Inspecting the fit visually and comparing the WAIC in this case we see that the regulation by *sigA* and *sigB* is plausible.

```{r, message=FALSE,warning=FALSE}
plot1 <- fitted_target_plot(fit_two_reg, data_two_reg)
plot2 <- fitted_target_observed_plot(fit_two_reg, data_two_reg)
plot_grid(plot1, plot2, ncol = 2)
regulated_table %>% filter(target == "purT") %>% mutate(waic_two_reg = get_waic_multiple_targets(fit_two_reg)) 


```

# Conclusions

TODO

# Acknowledgements

TODO
The Stan team, help at forums.

# Session Info
```{r}
sessionInfo()
```

# References
